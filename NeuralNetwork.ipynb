{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InteractiveShell.ast_node_interactivity=\"last_expr_or_assign\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv, sys\n",
    "import uproot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "import shap\n",
    "import tensorflow as tf\n",
    "import tkinter as tk\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# don't use these in notebook mode\n",
    "#from matplotlib.backends.backend_pdf import PdfPages\n",
    "#matplotlib.use(\"PDF\")\n",
    "import math\n",
    "import time\n",
    "from math import log, sqrt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()  # Normalized data to range from (0,1)\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_curve,\n",
    "    plot_precision_recall_curve,\n",
    "    average_precision_score,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    roc_auc_score,\n",
    "    precision_recall_curve,\n",
    "    confusion_matrix\n",
    ")\n",
    "from datetime import datetime\n",
    "\n",
    "# Checking if a GPU is available, not sure it will run in Jupyter\n",
    "status = len(tf.config.experimental.list_physical_devices(\"GPU\"))\n",
    "\n",
    "# If we need a random seed.\n",
    "seed = 42;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plotPR(x, y, t):\n",
    "    plt.subplot(411)\n",
    "    plt.plot(t, x[:-1], \"b--\", label=\"Precision\")\n",
    "    plt.plot(t, y[:-1], \"g-\", label=\"Recall\")\n",
    "    plt.ylim([0.00, 1.05])\n",
    "    plt.xlabel(\"Threshold\")\n",
    "    plt.title(\"Precision/Recall vs. Threshold Curve\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid()\n",
    "\n",
    "\n",
    "def plotROC(x, y, AUC):\n",
    "    plt.subplot(412)\n",
    "    plt.plot(x, y, lw=1, label=\"ROC (area = %0.6f)\" % (AUC))\n",
    "    plt.plot([0, 1], [0, 1], \"--\", color=(0.6, 0.6, 0.6), label=\"Luck\")\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"Receiver operating characteristic\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid()\n",
    "\n",
    "\n",
    "def getZPoisson(s, b, stat, syst):\n",
    "    \"\"\"\n",
    "    The significance for optimisation.\n",
    "    s: total number of signal events\n",
    "    b: total number of background events\n",
    "    stat: relative MC stat uncertainty for the total bkg.\n",
    "    syst: relative syst uncertainty on background\n",
    "    Note that the function already accounts for the sqrt(b)\n",
    "    uncertainty from the data, so we only need to pass in additional\n",
    "    stat and syst terms.  e.g. the stat term above is really only\n",
    "    characterizing the uncertainty due to limited MC statistics used\n",
    "    to estimate the background yield.\n",
    "    \"\"\"\n",
    "    n = s + b\n",
    "\n",
    "    # this is a relative uncertainty\n",
    "    sigma = math.sqrt(stat ** 2 + syst ** 2)\n",
    "\n",
    "    # turn into the total uncertainty\n",
    "    sigma = sigma * b\n",
    "\n",
    "    if s <= 0 or b <= 0:\n",
    "        return 0\n",
    "\n",
    "    factor1 = 0\n",
    "    factor2 = 0\n",
    "\n",
    "    if sigma < 0.01:\n",
    "        # In the limit where the total BG uncertainty is zero,\n",
    "        # this reduces to approximately s/sqrt(b)\n",
    "        factor1 = n * math.log((n / b))\n",
    "        factor2 = n - b\n",
    "    else:\n",
    "        factor1 = n * math.log((n * (b + sigma ** 2)) / ((b ** 2) + n * sigma ** 2))\n",
    "        factor2 = ((b ** 2) / (sigma ** 2)) * math.log(\n",
    "            1 + ((sigma ** 2) * (n - b)) / (b * (b + sigma ** 2))\n",
    "        )\n",
    "\n",
    "    signif = 0\n",
    "    try:\n",
    "        signif = math.sqrt(2 * (factor1 - factor2))\n",
    "    except ValueError:\n",
    "        signif = 0\n",
    "\n",
    "    return signif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block defines the branch names that we'll pull from the tree.  So we'll want to edit this to update to whatever variables we're using to train the NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Branches names of high/low level variables aka: features.\n",
    "HighLevel = [\n",
    "    \"numjet\",\n",
    "    \"numlep\",\n",
    "    \"btag\",\n",
    "    \"srap\",\n",
    "    \"cent\",\n",
    "    \"m_bb\",\n",
    "    \"h_b\",\n",
    "    \"mt1\",\n",
    "    \"mt2\",\n",
    "    \"mt3\",\n",
    "    \"dr1\",\n",
    "    \"dr2\",\n",
    "    \"dr3\",\n",
    "]\n",
    "\n",
    "### Low Level START -\n",
    "type = [\"flav\", \"pT\", \"eta\", \"phi\", \"b\", \"c\"]\n",
    "LeptonVAR = []\n",
    "JetVAR = []\n",
    "for i in range(4):\n",
    "    for j in range(3):\n",
    "        LeptonVAR.append(\"lepton\" + str(j + 1) + type[i])\n",
    "for i in range(1, 6):\n",
    "    for j in range(10):\n",
    "        JetVAR.append(\"jet\" + str(j + 1) + type[i])\n",
    "###                                               -END\n",
    "\n",
    "# Auto select feature set.\n",
    "phase = 3\n",
    "if phase == 1:\n",
    "    branches = sorted(HighLevel) + [\"weights\", \"truth\"]\n",
    "elif phase == 2:\n",
    "    branches = sorted(LeptonVAR + JetVAR) + [\"weights\", \"truth\"]\n",
    "elif phase == 3:\n",
    "    branches = sorted(HighLevel + JetVAR + LeptonVAR) + [\"weights\", \"truth\"]\n",
    "\n",
    "\n",
    "# Number of features.\n",
    "numBranches = len(branches) - 2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block reads in the data from the input files, using the branches specified above.  So we should update this section to use the right input files, etc.  Note there's also some scaling of the samples to get the weights right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data read from file.\n",
    "tree = \"OutputTree\"\n",
    "\n",
    "filepath=\"/data/users/mhance/tthh/\"\n",
    "\n",
    "signal = uproot.open(filepath+\"new_TTHH.root\")[tree]\n",
    "df_signal = signal.arrays(branches,library='pd')\n",
    "\n",
    "bkgTTBB = uproot.open(filepath+\"new_TTBB.root\")[tree]\n",
    "df_bkgTTBB = bkgTTBB.arrays(branches,library=\"pd\")\n",
    "\n",
    "bkgTTH = uproot.open(filepath+\"new_TTH.root\")[tree]\n",
    "df_bkgTTH = bkgTTH.arrays(branches,library=\"pd\")\n",
    "\n",
    "bkgTTZ = uproot.open(filepath+\"new_TTZ.root\")[tree]\n",
    "df_bkgTTZ = bkgTTZ.arrays(branches,library=\"pd\")\n",
    "\n",
    "df_background = pd.concat([df_bkgTTBB, df_bkgTTH, df_bkgTTZ])\n",
    "\n",
    "# The 3 backgrounds are concatenated we shuffle to make sure they are not sorted.\n",
    "shuffleBackground = shuffle(df_background, random_state=seed)\n",
    "\n",
    "# Signal and shuffle background data.\n",
    "rawdata = pd.concat([df_signal, shuffleBackground])\n",
    "\n",
    "X = rawdata.drop([\"weights\", \"truth\"], axis=1)\n",
    "\n",
    "# Normalized the data with a Gaussian distrubuition with 0 mean and unit variance.\n",
    "X = sc.fit_transform(X)\n",
    "\n",
    "# Signal\n",
    "scalefactor = 0.00232 * 0.608791\n",
    "sigw = rawdata[\"weights\"][: len(signal)] * scalefactor\n",
    "bkgw = rawdata[\"weights\"][len(signal) :]\n",
    "\n",
    "# Labeling data with 1's and 0's to distinguish.(1/positve/signal and 0/negative/background)\n",
    "# Truth Labels.\n",
    "y = np.concatenate((np.ones(len(df_signal)), np.zeros(len(shuffleBackground))))\n",
    "\n",
    "# Shuffle full data and split into train/test and validation set.\n",
    "X_dev, X_eval, y_dev, y_eval = train_test_split(\n",
    "    X, y, test_size=0.001, random_state=seed, stratify=y\n",
    ")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_dev, y_dev, test_size=0.2, random_state=seed, stratify=y_dev\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN model defined as a function.\n",
    "def build_model(network,RATE):\n",
    "\n",
    "    # Create a NN model. Barebones model with no layers.\n",
    "    model = Sequential()\n",
    "\n",
    "    # Best option for most NN.\n",
    "    opt = keras.optimizers.Nadam()\n",
    "\n",
    "    # Activation function other options possible.\n",
    "    act = \"relu\"  # Relu is 0 for negative values, linear for nonzero values.\n",
    "\n",
    "    # Use model.add() to add one layer at a time, 1st layer needs input shape, So we pass the 1st element of network.\n",
    "    # Dense Layers are fully connected and most common.\n",
    "\n",
    "    model.add(Dense(network[0], input_dim=numBranches))\n",
    "\n",
    "    # Loop through and add layers (1,(n-2)) where n is the number of layers. We end at n-2 because we start at 1 not zero and\n",
    "    # we  the input layer is added above with input dimension. Therefore we must remove 2 from layers.\n",
    "    for i in range(1, len(network) - 2):\n",
    "        model.add(Dense(network[i], activation=act))  # Hidden layers.\n",
    "        # Turning off nuerons of layer above in loop with probability = 1-r, so r = 0.25, then 75% of nerouns are kept.\n",
    "        model.add(Dropout(RATE, seed=seed))\n",
    "\n",
    "    # Last layer needs to have one neuron for a binary classification(BC) which yields from 0 to 1.\n",
    "    model.add(\n",
    "        Dense(network[-1], activation=\"sigmoid\")\n",
    "    )  # Output layer's activation function for BC needs to be sigmoid.\n",
    "\n",
    "    # Last step is compiling.\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=opt,\n",
    "        metrics=tf.keras.metrics.Precision(),\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_train_test(kModel, X_train, y_train, X_test, y_test, bins=30):\n",
    "    \"\"\"\n",
    "    This creates the signal and background distrubution.\n",
    "    \"\"\"\n",
    "    decisions = []\n",
    "    for X, y in ((X_train, y_train), (X_test, y_test)):\n",
    "        d1 = model.predict(X[y > 0.5]).ravel()  # signal\n",
    "        d2 = model.predict(X[y < 0.5]).ravel()  # background\n",
    "        decisions += [d1, d2]\n",
    "    low = min(np.min(d) for d in decisions)\n",
    "    high = max(np.max(d) for d in decisions)\n",
    "    low_high = array([low, high])\n",
    "\n",
    "    plt.subplot(212)\n",
    "    plt.hist(\n",
    "        decisions[0],\n",
    "        color=\"r\",\n",
    "        alpha=0.5,\n",
    "        range=low_high,\n",
    "        bins=bins,\n",
    "        histtype=\"stepfilled\",\n",
    "        density=True,\n",
    "        label=\"S (train)\",\n",
    "    )\n",
    "    plt.hist(\n",
    "        decisions[1],\n",
    "        color=\"b\",\n",
    "        alpha=0.5,\n",
    "        range=low_high,\n",
    "        bins=bins,\n",
    "        histtype=\"stepfilled\",\n",
    "        density=True,\n",
    "        label=\"B (train)\",\n",
    "    )\n",
    "\n",
    "    hist, bins = np.histogram(decisions[2], bins=bins, range=low_high, density=True)\n",
    "    scale = len(decisions[2]) / sum(hist)\n",
    "    err = np.sqrt(hist * scale) / scale\n",
    "\n",
    "    width = bins[1] - bins[0]\n",
    "    center = (bins[:-1] + bins[1:]) / 2\n",
    "    plt.errorbar(center, hist, yerr=err, fmt=\"o\", c=\"r\", label=\"S (test)\")\n",
    "\n",
    "    hist, bins = np.histogram(decisions[3], bins=bins, range=low_high, density=True)\n",
    "    scale = len(decisions[2]) / sum(hist)\n",
    "    err = np.sqrt(hist * scale) / scale\n",
    "\n",
    "    plt.errorbar(center, hist, yerr=err, fmt=\"o\", c=\"b\", label=\"B (test)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runNN(LAYER, BATCH, RATE):\n",
    "    \"\"\"\n",
    "    NN structure ex. [5,5,5,5,1] 4 layers with 5 neurons each and one output layer. LAYER value is\n",
    "    the number of hidden layers excluding the output layer. Each hidden layer will contain the same\n",
    "    amount of neurons (It is hard coded to be the number of features). The BATCH is the batch size,\n",
    "    powers of 2 are perfered but any positive number works. RATE is the drop out rate; so a RATE = .5\n",
    "    is half of the neurons being randomly turned off.\n",
    "    \"\"\"\n",
    "    network = []\n",
    "    numEpochs = 150  # Number of times the NN gets trained.\n",
    "    batchSize = BATCH\n",
    "    numLayers = LAYER\n",
    "    neurons = numBranches\n",
    "\n",
    "    # This creates a list that has the stucture of the NN.\n",
    "    for i in range(numLayers - 1):\n",
    "        network.append(neurons)\n",
    "    network.append(1)\n",
    "    numNeurons = sum(network)\n",
    "\n",
    "    # This is a conformation that the script is starting and the NN structure is displayed.\n",
    "    print(\"Script starting....\\n\", network)\n",
    "\n",
    "    # This tags the output files with either GPU or CPU.\n",
    "    if status == 1:\n",
    "        print(\"GPU\")\n",
    "        sufix = \".GPU\"\n",
    "    else:\n",
    "        sufix = \".CPU\"\n",
    "        print(\"CPU\")\n",
    "\n",
    "    # Start time for file name.\n",
    "    startTime = datetime.now()\n",
    "    pre = time.strftime(\"%Y.%m.%d_\") + time.strftime(\"%H.%M.%S.\")\n",
    "\n",
    "    # Filename for keras model to be saved as.\n",
    "    h5name = (\n",
    "        \"numLayers\"\n",
    "        + str(LAYER)\n",
    "        + \".numBranches\"\n",
    "        + str(neurons)\n",
    "        + \".batchSize\"\n",
    "        + str(BATCH)\n",
    "    )\n",
    "    modelName = \"data/\" + pre + h5name + sufix + \".h5\"\n",
    "\n",
    "    # Filename for plots to be identified by saved model.\n",
    "    figname = \"data/\" + pre + \".plots\"\n",
    "\n",
    "    # Using model and setting parameters.\n",
    "    model = build_model(network,RATE)\n",
    "\n",
    "    # This checkpoint is used for recovery of trained weights incase of interuption.\n",
    "    checkPointsCallBack = ModelCheckpoint(\"temp.h5\", save_best_only=True)\n",
    "\n",
    "    # This terminates early if the monitor does not see an improvement after a certain\n",
    "    # amount of epochs given by the patience.\n",
    "    earlyStopCallBack = EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=30, restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    # This is where the training starts.\n",
    "    kModel = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=numEpochs,\n",
    "        batch_size=batchSize,\n",
    "        validation_data=(X_test, y_test),\n",
    "        verbose=1,\n",
    "        callbacks=[earlyStopCallBack, checkPointsCallBack],\n",
    "    )\n",
    "    \n",
    "    return model,kModel,startTime,modelName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def storeModel(model,startTime,modelName,aucroc):        \n",
    "    # computes max signif\n",
    "    numbins = 100000\n",
    "    allScore = model.predict(X)\n",
    "    sigScore = model.predict(X[y > 0.5]).ravel()\n",
    "    bkgScore = model.predict(X[y < 0.5]).ravel()\n",
    "    sigSUM = len(sigScore)\n",
    "    bkgSUM = len(bkgScore)\n",
    "    xlimit = (0, 1)\n",
    "    tp = []\n",
    "    fp = []\n",
    "    hist, bins = np.histogram(sigScore, bins=numbins, range=xlimit, density=False)\n",
    "    count = 0\n",
    "    for i in range(numbins - 1, -1, -1):\n",
    "        count += hist[i] / sigSUM\n",
    "        tp.append(count)\n",
    "    hist, bins = np.histogram(bkgScore, bins=numbins, range=xlimit, density=False)\n",
    "    count = 0\n",
    "    for j in range(numbins - 1, -1, -1):\n",
    "        count += hist[j] / bkgSUM\n",
    "        fp.append(count)\n",
    "    area = auc(fp, tp)\n",
    "    xplot = tp\n",
    "    yplot = fp\n",
    "    # computes max signif\n",
    "    sigSUM = len(sigScore) * scalefactor\n",
    "    tp = np.array(tp) * sigSUM\n",
    "    fp = np.array(fp) * bkgSUM\n",
    "    syst = 0.0\n",
    "    stat = 0.0\n",
    "    maxsignif = 0.0\n",
    "    maxs = 0\n",
    "    maxb = 0\n",
    "    bincounter = numbins - 1\n",
    "    bincountatmaxsignif = 999\n",
    "    for t, f in zip(tp, fp):\n",
    "        signif = getZPoisson(t, f, stat, syst)\n",
    "        if f >= 10 and signif > maxsignif:\n",
    "            maxsignif = signif\n",
    "            maxs = t\n",
    "            maxb = f\n",
    "            bincountatmaxsignif = bincounter\n",
    "            score = bincountatmaxsignif / numbins\n",
    "        bincounter -= 1\n",
    "    print(\n",
    "        \"\\n Score = %6.3f\\n Signif = %5.2f\\n nsig = %d\\n nbkg = %d\\n\"\n",
    "        % (score, maxsignif, maxs, maxb)\n",
    "    )\n",
    "    runtime = datetime.now() - startTime\n",
    "    areaUnderCurve = \"{:.4f}\".format(aucroc)\n",
    "    maxsignif = \"{:5.2f}\".format(maxsignif)\n",
    "    # This is the predicted score. Values range between [0,1]\n",
    "    y_predicted = model.predict(X_test)\n",
    "    # The score is rounded; values are 0 or 1.  This isn't actually used?\n",
    "    y_predicted_round = [1 * (x[0] >= 0.5) for x in y_predicted]\n",
    "    average_precision = average_precision_score(y_test, y_predicted)\n",
    "    avgPer = \"{0:0.4f}\".format(average_precision)\n",
    "    score = \"{0:6.3f}\".format(score)\n",
    "    maxs = \"%10d\" % (maxs)\n",
    "    maxb = \"%10d\" % (maxb)\n",
    "    cm = confusion_matrix(y_test, y_predicted_round)\n",
    "    CM = [cm[0][0], cm[0][1]], [cm[1, 0], cm[1, 1]]\n",
    "    modelParam = [\n",
    "        \"FileName\",\n",
    "        \"ConfusionMatrix [TP FP] [FN TN]\",\n",
    "        \"Run Time\",\n",
    "        \"AUC\",\n",
    "        \"Avg.P\",\n",
    "        \"Score\",\n",
    "        \"Max Signif\",\n",
    "        \"nsig\",\n",
    "        \"nbkg\",\n",
    "    ]\n",
    "    df = pd.DataFrame(\n",
    "        np.array(\n",
    "            [\n",
    "                [\n",
    "                    modelName[5:],\n",
    "                    CM,\n",
    "                    runtime,\n",
    "                    areaUnderCurve,\n",
    "                    avgPer,\n",
    "                    score,\n",
    "                    maxsignif,\n",
    "                    maxs,\n",
    "                    maxb,\n",
    "                ]\n",
    "            ]\n",
    "        ),\n",
    "        columns=modelParam,\n",
    "    )\n",
    "    df.to_csv(\"csv/testelep2.csv\", mode=\"a\", header=False, index=False)\n",
    "    print(df.to_string(justify=\"left\", columns=modelParam, header=True, index=False))\n",
    "    print(\"Saving model.....\")\n",
    "    print(\"old auc: \\n\", aucroc, \"\\n new auc\", areaUnderCurve)\n",
    "    model.save(modelName)  # Save Model as a HDF5 filein Data folder\n",
    "    print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkTraining(model,kModel):\n",
    "    # This is the predicted score. Values range between [0,1]\n",
    "    y_predicted = model.predict(X_test)\n",
    "\n",
    "    # Prediction, fpr,tpr and threshold values for ROC.\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_predicted)\n",
    "    aucroc = auc(fpr, tpr)\n",
    "    precision, recall, thresRecall = precision_recall_curve(y_test, y_predicted)\n",
    "\n",
    "    plt.xlabel(\"Score\")\n",
    "    plt.ylabel(\"Distribution\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.subplot(211)\n",
    "    plt.plot(fpr, tpr, \"r-\", label=\"ROC (area = %0.6f)\" % (aucroc))\n",
    "    plt.plot([0, 1], [0, 1], \"--\", color=(0.6, 0.6, 0.6), label=\"Luck\")\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"Receiver operating characteristic\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # AUC\n",
    "    \n",
    "    # plot1 = plt.figure(1)\n",
    "    # plotROC(fpr, tpr, aucroc)\n",
    "    # plotPR(precision,recall,thresRecall)\n",
    "    compare_train_test(kModel, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    if 0:\n",
    "        # This plots the important features.\n",
    "        plot2 = plt.figure(2)\n",
    "        backgrounds = X_train[np.random.choice(X_train.shape[0], 100, replace=False)]\n",
    "        explainer = shap.DeepExplainer(model, backgrounds)\n",
    "        shap_values = explainer.shap_values(X_test)\n",
    "        shap.summary_plot(\n",
    "            shap_values,\n",
    "            X_train,\n",
    "            plot_type=\"bar\",\n",
    "            feature_names=branches[:-1],\n",
    "            max_display=25,\n",
    "        )\n",
    "    return aucroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script starting....\n",
      " [75, 75, 1]\n",
      "CPU\n",
      "Epoch 1/150\n",
      "6618/6618 [==============================] - 7s 1ms/step - loss: 0.2144 - precision: 0.7374 - val_loss: 0.2009 - val_precision: 0.7763\n",
      "Epoch 2/150\n",
      "6618/6618 [==============================] - 6s 980us/step - loss: 0.2004 - precision: 0.7939 - val_loss: 0.2083 - val_precision: 0.7009\n",
      "Epoch 3/150\n",
      "6618/6618 [==============================] - 6s 980us/step - loss: 0.1995 - precision: 0.7971 - val_loss: 0.2007 - val_precision: 0.7430\n",
      "Epoch 4/150\n",
      "6618/6618 [==============================] - 6s 982us/step - loss: 0.1992 - precision: 0.7975 - val_loss: 0.2007 - val_precision: 0.8209\n",
      "Epoch 5/150\n",
      "6618/6618 [==============================] - 6s 979us/step - loss: 0.1988 - precision: 0.7990 - val_loss: 0.1973 - val_precision: 0.7982\n",
      "Epoch 6/150\n",
      "6618/6618 [==============================] - 7s 986us/step - loss: 0.1988 - precision: 0.7998 - val_loss: 0.1982 - val_precision: 0.7709\n",
      "Epoch 7/150\n",
      "6618/6618 [==============================] - 6s 979us/step - loss: 0.1983 - precision: 0.8001 - val_loss: 0.2002 - val_precision: 0.8135\n",
      "Epoch 8/150\n",
      "6618/6618 [==============================] - 6s 981us/step - loss: 0.1980 - precision: 0.8011 - val_loss: 0.1983 - val_precision: 0.8286\n",
      "Epoch 9/150\n",
      "6618/6618 [==============================] - 7s 983us/step - loss: 0.1981 - precision: 0.8022 - val_loss: 0.1975 - val_precision: 0.8397\n",
      "Epoch 10/150\n",
      "6618/6618 [==============================] - 6s 981us/step - loss: 0.1979 - precision: 0.8024 - val_loss: 0.1973 - val_precision: 0.7959\n",
      "Epoch 11/150\n",
      "6618/6618 [==============================] - 6s 977us/step - loss: 0.1978 - precision: 0.8031 - val_loss: 0.1966 - val_precision: 0.7788\n",
      "Epoch 12/150\n",
      "6618/6618 [==============================] - 6s 981us/step - loss: 0.1976 - precision: 0.8032 - val_loss: 0.1964 - val_precision: 0.8362\n",
      "Epoch 13/150\n",
      "6618/6618 [==============================] - 6s 981us/step - loss: 0.1975 - precision: 0.8036 - val_loss: 0.1966 - val_precision: 0.8152\n",
      "Epoch 14/150\n",
      "6618/6618 [==============================] - 6s 980us/step - loss: 0.1975 - precision: 0.8040 - val_loss: 0.1956 - val_precision: 0.8253\n",
      "Epoch 15/150\n",
      "6618/6618 [==============================] - 7s 982us/step - loss: 0.1975 - precision: 0.8043 - val_loss: 0.1953 - val_precision: 0.8073\n",
      "Epoch 16/150\n",
      "6618/6618 [==============================] - 7s 986us/step - loss: 0.1974 - precision: 0.8040 - val_loss: 0.1969 - val_precision: 0.8082\n",
      "Epoch 17/150\n",
      "6618/6618 [==============================] - 6s 982us/step - loss: 0.1973 - precision: 0.8049 - val_loss: 0.1954 - val_precision: 0.8200\n",
      "Epoch 18/150\n",
      "6618/6618 [==============================] - 7s 986us/step - loss: 0.1972 - precision: 0.8044 - val_loss: 0.1958 - val_precision: 0.8342\n",
      "Epoch 19/150\n",
      "6618/6618 [==============================] - 6s 982us/step - loss: 0.1973 - precision: 0.8048 - val_loss: 0.1956 - val_precision: 0.7988\n",
      "Epoch 20/150\n",
      "6618/6618 [==============================] - 6s 979us/step - loss: 0.1973 - precision: 0.8051 - val_loss: 0.1954 - val_precision: 0.8288\n",
      "Epoch 21/150\n",
      "6618/6618 [==============================] - 7s 985us/step - loss: 0.1971 - precision: 0.8056 - val_loss: 0.1968 - val_precision: 0.8087\n",
      "Epoch 22/150\n",
      "6618/6618 [==============================] - 6s 978us/step - loss: 0.1971 - precision: 0.8055 - val_loss: 0.1980 - val_precision: 0.8090\n",
      "Epoch 23/150\n",
      "6618/6618 [==============================] - 6s 982us/step - loss: 0.1971 - precision: 0.8065 - val_loss: 0.1968 - val_precision: 0.7901\n",
      "Epoch 24/150\n",
      "6618/6618 [==============================] - 6s 982us/step - loss: 0.1970 - precision: 0.8056 - val_loss: 0.1952 - val_precision: 0.8169\n",
      "Epoch 25/150\n",
      "6618/6618 [==============================] - 7s 983us/step - loss: 0.1970 - precision: 0.8052 - val_loss: 0.1958 - val_precision: 0.8342\n",
      "Epoch 26/150\n",
      "6618/6618 [==============================] - 6s 979us/step - loss: 0.1969 - precision: 0.8061 - val_loss: 0.1946 - val_precision: 0.8026\n",
      "Epoch 27/150\n",
      "6618/6618 [==============================] - 6s 979us/step - loss: 0.1970 - precision: 0.8063 - val_loss: 0.1949 - val_precision: 0.8032\n",
      "Epoch 28/150\n",
      "6618/6618 [==============================] - 6s 979us/step - loss: 0.1969 - precision: 0.8059 - val_loss: 0.1946 - val_precision: 0.8150\n",
      "Epoch 29/150\n",
      "6618/6618 [==============================] - 6s 981us/step - loss: 0.1969 - precision: 0.8065 - val_loss: 0.1947 - val_precision: 0.8241\n",
      "Epoch 30/150\n",
      "6618/6618 [==============================] - 6s 978us/step - loss: 0.1969 - precision: 0.8064 - val_loss: 0.1951 - val_precision: 0.7931\n",
      "Epoch 31/150\n",
      "6618/6618 [==============================] - 6s 979us/step - loss: 0.1969 - precision: 0.8070 - val_loss: 0.1947 - val_precision: 0.8026\n",
      "Epoch 32/150\n",
      "6618/6618 [==============================] - 7s 982us/step - loss: 0.1969 - precision: 0.8068 - val_loss: 0.1944 - val_precision: 0.8153\n",
      "Epoch 33/150\n",
      "6618/6618 [==============================] - 6s 980us/step - loss: 0.1969 - precision: 0.8070 - val_loss: 0.1947 - val_precision: 0.8307\n",
      "Epoch 34/150\n",
      "6618/6618 [==============================] - 6s 982us/step - loss: 0.1969 - precision: 0.8067 - val_loss: 0.1947 - val_precision: 0.8265\n",
      "Epoch 35/150\n",
      "6618/6618 [==============================] - 7s 983us/step - loss: 0.1968 - precision: 0.8070 - val_loss: 0.1947 - val_precision: 0.8210\n",
      "Epoch 36/150\n",
      "6618/6618 [==============================] - 7s 984us/step - loss: 0.1969 - precision: 0.8066 - val_loss: 0.1944 - val_precision: 0.8001\n",
      "Epoch 37/150\n",
      "6618/6618 [==============================] - 6s 980us/step - loss: 0.1967 - precision: 0.8070 - val_loss: 0.1963 - val_precision: 0.8125\n",
      "Epoch 38/150\n",
      "6618/6618 [==============================] - 7s 982us/step - loss: 0.1968 - precision: 0.8070 - val_loss: 0.1947 - val_precision: 0.7988\n",
      "Epoch 39/150\n",
      "6618/6618 [==============================] - 7s 986us/step - loss: 0.1968 - precision: 0.8074 - val_loss: 0.1945 - val_precision: 0.8084\n",
      "Epoch 40/150\n",
      "6618/6618 [==============================] - 7s 986us/step - loss: 0.1968 - precision: 0.8073 - val_loss: 0.1950 - val_precision: 0.7801\n",
      "Epoch 41/150\n",
      "6618/6618 [==============================] - 7s 983us/step - loss: 0.1968 - precision: 0.8075 - val_loss: 0.1946 - val_precision: 0.7987\n",
      "Epoch 42/150\n",
      "6618/6618 [==============================] - 7s 984us/step - loss: 0.1968 - precision: 0.8073 - val_loss: 0.1944 - val_precision: 0.8104\n",
      "Epoch 43/150\n",
      "6618/6618 [==============================] - 6s 979us/step - loss: 0.1967 - precision: 0.8073 - val_loss: 0.1944 - val_precision: 0.8052\n",
      "Epoch 44/150\n",
      "6618/6618 [==============================] - 6s 974us/step - loss: 0.1967 - precision: 0.8074 - val_loss: 0.1981 - val_precision: 0.7689\n",
      "Epoch 45/150\n",
      "6618/6618 [==============================] - 7s 985us/step - loss: 0.1968 - precision: 0.8077 - val_loss: 0.1946 - val_precision: 0.8020\n",
      "Epoch 46/150\n",
      "6618/6618 [==============================] - 7s 986us/step - loss: 0.1966 - precision: 0.8080 - val_loss: 0.1943 - val_precision: 0.8190\n",
      "Epoch 47/150\n",
      "6618/6618 [==============================] - 7s 988us/step - loss: 0.1967 - precision: 0.8071 - val_loss: 0.1959 - val_precision: 0.8027\n",
      "Epoch 48/150\n",
      "6618/6618 [==============================] - 7s 982us/step - loss: 0.1967 - precision: 0.8072 - val_loss: 0.1948 - val_precision: 0.7903\n",
      "Epoch 49/150\n",
      "6618/6618 [==============================] - 6s 978us/step - loss: 0.1966 - precision: 0.8077 - val_loss: 0.1943 - val_precision: 0.8130\n",
      "Epoch 50/150\n",
      "6618/6618 [==============================] - 6s 981us/step - loss: 0.1967 - precision: 0.8077 - val_loss: 0.1944 - val_precision: 0.8073\n",
      "Epoch 51/150\n",
      "6618/6618 [==============================] - 6s 980us/step - loss: 0.1967 - precision: 0.8078 - val_loss: 0.1950 - val_precision: 0.7937\n",
      "Epoch 52/150\n",
      "6618/6618 [==============================] - 6s 979us/step - loss: 0.1967 - precision: 0.8078 - val_loss: 0.1946 - val_precision: 0.8005\n",
      "Epoch 53/150\n",
      "6618/6618 [==============================] - 6s 980us/step - loss: 0.1965 - precision: 0.8080 - val_loss: 0.1947 - val_precision: 0.8197\n",
      "Epoch 54/150\n",
      "6618/6618 [==============================] - 6s 974us/step - loss: 0.1966 - precision: 0.8078 - val_loss: 0.1948 - val_precision: 0.8372\n",
      "Epoch 55/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6618/6618 [==============================] - 6s 973us/step - loss: 0.1966 - precision: 0.8081 - val_loss: 0.1944 - val_precision: 0.8086\n",
      "Epoch 56/150\n",
      "6618/6618 [==============================] - 6s 981us/step - loss: 0.1966 - precision: 0.8077 - val_loss: 0.1943 - val_precision: 0.8271\n",
      "Epoch 57/150\n",
      "6618/6618 [==============================] - 6s 978us/step - loss: 0.1966 - precision: 0.8077 - val_loss: 0.1960 - val_precision: 0.7970\n",
      "Epoch 58/150\n",
      "6618/6618 [==============================] - 6s 981us/step - loss: 0.1966 - precision: 0.8077 - val_loss: 0.1951 - val_precision: 0.8333\n",
      "Epoch 59/150\n",
      "6618/6618 [==============================] - 7s 983us/step - loss: 0.1966 - precision: 0.8078 - val_loss: 0.1944 - val_precision: 0.8285\n",
      "Epoch 60/150\n",
      "6618/6618 [==============================] - 6s 979us/step - loss: 0.1967 - precision: 0.8080 - val_loss: 0.1952 - val_precision: 0.8125\n",
      "Epoch 61/150\n",
      "6618/6618 [==============================] - 6s 981us/step - loss: 0.1967 - precision: 0.8080 - val_loss: 0.1954 - val_precision: 0.8357\n",
      "Epoch 62/150\n",
      "6618/6618 [==============================] - 7s 987us/step - loss: 0.1966 - precision: 0.8075 - val_loss: 0.1953 - val_precision: 0.8334\n",
      "Epoch 63/150\n",
      "6618/6618 [==============================] - 6s 979us/step - loss: 0.1965 - precision: 0.8085 - val_loss: 0.1938 - val_precision: 0.8090\n",
      "Epoch 64/150\n",
      "6618/6618 [==============================] - 6s 976us/step - loss: 0.1966 - precision: 0.8081 - val_loss: 0.1940 - val_precision: 0.8067\n",
      "Epoch 65/150\n",
      "6618/6618 [==============================] - 6s 980us/step - loss: 0.1966 - precision: 0.8082 - val_loss: 0.1954 - val_precision: 0.8340\n",
      "Epoch 66/150\n",
      "6618/6618 [==============================] - 7s 985us/step - loss: 0.1966 - precision: 0.8087 - val_loss: 0.1953 - val_precision: 0.7811\n",
      "Epoch 67/150\n",
      "6618/6618 [==============================] - 6s 980us/step - loss: 0.1965 - precision: 0.8083 - val_loss: 0.1941 - val_precision: 0.8192\n",
      "Epoch 68/150\n",
      "6618/6618 [==============================] - 6s 980us/step - loss: 0.1967 - precision: 0.8079 - val_loss: 0.1940 - val_precision: 0.8168\n",
      "Epoch 69/150\n",
      "6618/6618 [==============================] - 7s 983us/step - loss: 0.1965 - precision: 0.8085 - val_loss: 0.1944 - val_precision: 0.8330\n",
      "Epoch 70/150\n",
      "6618/6618 [==============================] - 6s 979us/step - loss: 0.1966 - precision: 0.8084 - val_loss: 0.1953 - val_precision: 0.7857\n",
      "Epoch 71/150\n",
      "6618/6618 [==============================] - 7s 983us/step - loss: 0.1966 - precision: 0.8087 - val_loss: 0.1944 - val_precision: 0.8236\n",
      "Epoch 72/150\n",
      "6618/6618 [==============================] - 7s 984us/step - loss: 0.1966 - precision: 0.8084 - val_loss: 0.1944 - val_precision: 0.8005\n",
      "Epoch 73/150\n",
      "6618/6618 [==============================] - 6s 982us/step - loss: 0.1966 - precision: 0.8084 - val_loss: 0.1942 - val_precision: 0.8135\n",
      "Epoch 74/150\n",
      "6618/6618 [==============================] - 6s 981us/step - loss: 0.1965 - precision: 0.8082 - val_loss: 0.1956 - val_precision: 0.8065\n",
      "Epoch 75/150\n",
      "6618/6618 [==============================] - 6s 978us/step - loss: 0.1967 - precision: 0.8092 - val_loss: 0.1939 - val_precision: 0.8169\n",
      "Epoch 76/150\n",
      "6618/6618 [==============================] - 7s 987us/step - loss: 0.1965 - precision: 0.8084 - val_loss: 0.1944 - val_precision: 0.8350\n",
      "Epoch 77/150\n",
      "6618/6618 [==============================] - 6s 974us/step - loss: 0.1965 - precision: 0.8089 - val_loss: 0.1939 - val_precision: 0.8126\n",
      "Epoch 78/150\n",
      "6618/6618 [==============================] - 6s 979us/step - loss: 0.1965 - precision: 0.8089 - val_loss: 0.1942 - val_precision: 0.8245\n",
      "Epoch 79/150\n",
      "6618/6618 [==============================] - 6s 982us/step - loss: 0.1966 - precision: 0.8088 - val_loss: 0.1938 - val_precision: 0.8238\n",
      "Epoch 80/150\n",
      "6618/6618 [==============================] - 6s 978us/step - loss: 0.1965 - precision: 0.8088 - val_loss: 0.1949 - val_precision: 0.8281\n",
      "Epoch 81/150\n",
      "6618/6618 [==============================] - 7s 987us/step - loss: 0.1966 - precision: 0.8081 - val_loss: 0.1940 - val_precision: 0.8095\n",
      "Epoch 82/150\n",
      "6618/6618 [==============================] - 6s 982us/step - loss: 0.1965 - precision: 0.8087 - val_loss: 0.1940 - val_precision: 0.8168\n",
      "Epoch 83/150\n",
      "6618/6618 [==============================] - 6s 976us/step - loss: 0.1964 - precision: 0.8088 - val_loss: 0.1956 - val_precision: 0.8197\n",
      "Epoch 84/150\n",
      "6618/6618 [==============================] - 7s 984us/step - loss: 0.1966 - precision: 0.8088 - val_loss: 0.1938 - val_precision: 0.8044\n",
      "Epoch 85/150\n",
      "6618/6618 [==============================] - 7s 986us/step - loss: 0.1966 - precision: 0.8089 - val_loss: 0.1946 - val_precision: 0.8309\n",
      "Epoch 86/150\n",
      "6618/6618 [==============================] - 7s 984us/step - loss: 0.1965 - precision: 0.8090 - val_loss: 0.1952 - val_precision: 0.8226\n",
      "Epoch 87/150\n",
      "6618/6618 [==============================] - 7s 984us/step - loss: 0.1965 - precision: 0.8092 - val_loss: 0.1943 - val_precision: 0.8159\n",
      "Epoch 88/150\n",
      "6618/6618 [==============================] - 6s 976us/step - loss: 0.1966 - precision: 0.8088 - val_loss: 0.1941 - val_precision: 0.7996\n",
      "Epoch 89/150\n",
      "6618/6618 [==============================] - 6s 979us/step - loss: 0.1965 - precision: 0.8094 - val_loss: 0.1940 - val_precision: 0.8181\n",
      "Epoch 90/150\n",
      "6618/6618 [==============================] - 7s 983us/step - loss: 0.1965 - precision: 0.8091 - val_loss: 0.1938 - val_precision: 0.8171\n",
      "Epoch 91/150\n",
      "6618/6618 [==============================] - 6s 975us/step - loss: 0.1966 - precision: 0.8092 - val_loss: 0.1938 - val_precision: 0.8185\n",
      "Epoch 92/150\n",
      "6618/6618 [==============================] - 7s 982us/step - loss: 0.1965 - precision: 0.8089 - val_loss: 0.1941 - val_precision: 0.8177\n",
      "Epoch 93/150\n",
      "6618/6618 [==============================] - 6s 981us/step - loss: 0.1966 - precision: 0.8090 - val_loss: 0.1940 - val_precision: 0.8276\n",
      "Epoch 94/150\n",
      "6618/6618 [==============================] - 7s 985us/step - loss: 0.1966 - precision: 0.8090 - val_loss: 0.1940 - val_precision: 0.7977\n",
      "Epoch 95/150\n",
      "6618/6618 [==============================] - 7s 984us/step - loss: 0.1965 - precision: 0.8093 - val_loss: 0.1939 - val_precision: 0.8159\n",
      "Epoch 96/150\n",
      "6618/6618 [==============================] - 6s 979us/step - loss: 0.1966 - precision: 0.8091 - val_loss: 0.1940 - val_precision: 0.8357\n",
      "Epoch 97/150\n",
      "6618/6618 [==============================] - 6s 979us/step - loss: 0.1965 - precision: 0.8094 - val_loss: 0.1940 - val_precision: 0.8143\n",
      "Epoch 98/150\n",
      "6618/6618 [==============================] - 6s 980us/step - loss: 0.1965 - precision: 0.8095 - val_loss: 0.1938 - val_precision: 0.8050\n",
      "Epoch 99/150\n",
      "6618/6618 [==============================] - 6s 981us/step - loss: 0.1965 - precision: 0.8093 - val_loss: 0.1938 - val_precision: 0.8029\n",
      "Epoch 100/150\n",
      "6618/6618 [==============================] - 7s 985us/step - loss: 0.1965 - precision: 0.8094 - val_loss: 0.1938 - val_precision: 0.8269\n",
      "Epoch 101/150\n",
      "6618/6618 [==============================] - 6s 980us/step - loss: 0.1965 - precision: 0.8088 - val_loss: 0.1939 - val_precision: 0.8231\n",
      "Epoch 102/150\n",
      "6618/6618 [==============================] - 7s 984us/step - loss: 0.1964 - precision: 0.8094 - val_loss: 0.1948 - val_precision: 0.8159\n",
      "Epoch 103/150\n",
      "6618/6618 [==============================] - 6s 982us/step - loss: 0.1964 - precision: 0.8092 - val_loss: 0.1934 - val_precision: 0.8090\n",
      "Epoch 104/150\n",
      "6618/6618 [==============================] - 6s 981us/step - loss: 0.1966 - precision: 0.8098 - val_loss: 0.1934 - val_precision: 0.8164\n",
      "Epoch 105/150\n",
      "6618/6618 [==============================] - 6s 981us/step - loss: 0.1965 - precision: 0.8092 - val_loss: 0.1940 - val_precision: 0.8251\n",
      "Epoch 106/150\n",
      "6618/6618 [==============================] - 7s 986us/step - loss: 0.1965 - precision: 0.8102 - val_loss: 0.1943 - val_precision: 0.7855\n",
      "Epoch 107/150\n",
      "6618/6618 [==============================] - 6s 978us/step - loss: 0.1965 - precision: 0.8094 - val_loss: 0.1934 - val_precision: 0.8137\n",
      "Epoch 108/150\n",
      "6618/6618 [==============================] - 6s 981us/step - loss: 0.1965 - precision: 0.8097 - val_loss: 0.1940 - val_precision: 0.8134\n",
      "Epoch 109/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6618/6618 [==============================] - 7s 987us/step - loss: 0.1966 - precision: 0.8090 - val_loss: 0.1941 - val_precision: 0.8057\n",
      "Epoch 110/150\n",
      "6618/6618 [==============================] - 7s 984us/step - loss: 0.1963 - precision: 0.8094 - val_loss: 0.1948 - val_precision: 0.8128\n",
      "Epoch 111/150\n",
      "6618/6618 [==============================] - 6s 981us/step - loss: 0.1967 - precision: 0.8096 - val_loss: 0.1935 - val_precision: 0.8105\n",
      "Epoch 112/150\n",
      "6618/6618 [==============================] - 6s 981us/step - loss: 0.1965 - precision: 0.8089 - val_loss: 0.1937 - val_precision: 0.8266\n",
      "Epoch 113/150\n",
      "6618/6618 [==============================] - 6s 981us/step - loss: 0.1965 - precision: 0.8093 - val_loss: 0.1954 - val_precision: 0.7769\n",
      "Epoch 114/150\n",
      "6618/6618 [==============================] - 6s 977us/step - loss: 0.1961 - precision: 0.8092 - val_loss: 0.1942 - val_precision: 0.7943\n",
      "Epoch 115/150\n",
      "6618/6618 [==============================] - 6s 979us/step - loss: 0.1966 - precision: 0.8097 - val_loss: 0.1943 - val_precision: 0.7857\n",
      "Epoch 116/150\n",
      "6618/6618 [==============================] - 6s 976us/step - loss: 0.1967 - precision: 0.8100 - val_loss: 0.1935 - val_precision: 0.8100\n",
      "Epoch 117/150\n",
      "6618/6618 [==============================] - 6s 979us/step - loss: 0.1965 - precision: 0.8100 - val_loss: 0.1937 - val_precision: 0.8186\n",
      "Epoch 118/150\n",
      "6618/6618 [==============================] - 6s 976us/step - loss: 0.1966 - precision: 0.8093 - val_loss: 0.1937 - val_precision: 0.8327\n",
      "Epoch 119/150\n",
      "6618/6618 [==============================] - 6s 978us/step - loss: 0.1967 - precision: 0.8095 - val_loss: 0.1935 - val_precision: 0.8120\n",
      "Epoch 120/150\n",
      "6618/6618 [==============================] - 6s 978us/step - loss: 0.1964 - precision: 0.8098 - val_loss: 0.1947 - val_precision: 0.8172\n",
      "Epoch 121/150\n",
      "6618/6618 [==============================] - 6s 980us/step - loss: 0.1967 - precision: 0.8094 - val_loss: 0.1940 - val_precision: 0.8216\n",
      "Epoch 122/150\n",
      "6618/6618 [==============================] - 6s 982us/step - loss: 0.1964 - precision: 0.8102 - val_loss: 0.1958 - val_precision: 0.8376\n",
      "Epoch 123/150\n",
      "6618/6618 [==============================] - 6s 977us/step - loss: 0.1967 - precision: 0.8096 - val_loss: 0.1948 - val_precision: 0.7900\n",
      "Epoch 124/150\n",
      "6618/6618 [==============================] - 6s 982us/step - loss: 0.1965 - precision: 0.8104 - val_loss: 0.1933 - val_precision: 0.8227\n",
      "Epoch 125/150\n",
      "6618/6618 [==============================] - 7s 989us/step - loss: 0.1965 - precision: 0.8095 - val_loss: 0.1949 - val_precision: 0.8205\n",
      "Epoch 126/150\n",
      "6618/6618 [==============================] - 6s 977us/step - loss: 0.1966 - precision: 0.8100 - val_loss: 0.1937 - val_precision: 0.7967\n",
      "Epoch 127/150\n",
      "6618/6618 [==============================] - 7s 984us/step - loss: 0.1966 - precision: 0.8101 - val_loss: 0.1936 - val_precision: 0.8172\n",
      "Epoch 128/150\n",
      "6618/6618 [==============================] - 6s 978us/step - loss: 0.1966 - precision: 0.8096 - val_loss: 0.1935 - val_precision: 0.8251\n",
      "Epoch 129/150\n",
      "6618/6618 [==============================] - 6s 976us/step - loss: 0.1966 - precision: 0.8095 - val_loss: 0.1937 - val_precision: 0.8343\n",
      "Epoch 130/150\n",
      "6618/6618 [==============================] - 7s 986us/step - loss: 0.1967 - precision: 0.8095 - val_loss: 0.1936 - val_precision: 0.7991\n",
      "Epoch 131/150\n",
      "6618/6618 [==============================] - 6s 980us/step - loss: 0.1966 - precision: 0.8099 - val_loss: 0.1946 - val_precision: 0.8312\n",
      "Epoch 132/150\n",
      "6618/6618 [==============================] - 6s 980us/step - loss: 0.1963 - precision: 0.8099 - val_loss: 0.1934 - val_precision: 0.8064\n",
      "Epoch 133/150\n",
      "6618/6618 [==============================] - 6s 982us/step - loss: 0.1966 - precision: 0.8099 - val_loss: 0.1947 - val_precision: 0.8101\n",
      "Epoch 134/150\n",
      "6618/6618 [==============================] - 6s 982us/step - loss: 0.1965 - precision: 0.8101 - val_loss: 0.1941 - val_precision: 0.8060\n",
      "Epoch 135/150\n",
      "6618/6618 [==============================] - 6s 978us/step - loss: 0.1966 - precision: 0.8099 - val_loss: 0.1934 - val_precision: 0.8200\n",
      "Epoch 136/150\n",
      "6618/6618 [==============================] - 7s 983us/step - loss: 0.1967 - precision: 0.8102 - val_loss: 0.1937 - val_precision: 0.8032\n",
      "Epoch 137/150\n",
      "6618/6618 [==============================] - 6s 975us/step - loss: 0.1965 - precision: 0.8100 - val_loss: 0.1941 - val_precision: 0.8020\n",
      "Epoch 138/150\n",
      "6618/6618 [==============================] - 6s 977us/step - loss: 0.1966 - precision: 0.8097 - val_loss: 0.1948 - val_precision: 0.8001\n",
      "Epoch 139/150\n",
      "6618/6618 [==============================] - 6s 975us/step - loss: 0.1965 - precision: 0.8101 - val_loss: 0.1939 - val_precision: 0.8216\n",
      "Epoch 140/150\n",
      "6618/6618 [==============================] - 6s 976us/step - loss: 0.1966 - precision: 0.8096 - val_loss: 0.1936 - val_precision: 0.8272\n",
      "Epoch 141/150\n",
      "6618/6618 [==============================] - 6s 977us/step - loss: 0.1966 - precision: 0.8097 - val_loss: 0.1936 - val_precision: 0.8025\n",
      "Epoch 142/150\n",
      "6618/6618 [==============================] - 6s 979us/step - loss: 0.1967 - precision: 0.8106 - val_loss: 0.1936 - val_precision: 0.8152\n",
      "Epoch 143/150\n",
      "6618/6618 [==============================] - 7s 984us/step - loss: 0.1966 - precision: 0.8105 - val_loss: 0.1932 - val_precision: 0.8206\n",
      "Epoch 144/150\n",
      "6618/6618 [==============================] - 6s 980us/step - loss: 0.1966 - precision: 0.8097 - val_loss: 0.1934 - val_precision: 0.8101\n",
      "Epoch 145/150\n",
      "6618/6618 [==============================] - 7s 982us/step - loss: 0.1965 - precision: 0.8101 - val_loss: 0.1939 - val_precision: 0.8253\n",
      "Epoch 146/150\n",
      "6618/6618 [==============================] - 6s 981us/step - loss: 0.1968 - precision: 0.8104 - val_loss: 0.1934 - val_precision: 0.8186\n",
      "Epoch 147/150\n",
      "6618/6618 [==============================] - 6s 981us/step - loss: 0.1966 - precision: 0.8102 - val_loss: 0.1937 - val_precision: 0.8128\n",
      "Epoch 148/150\n",
      "6618/6618 [==============================] - 6s 980us/step - loss: 0.1966 - precision: 0.8099 - val_loss: 0.1941 - val_precision: 0.8107\n",
      "Epoch 149/150\n",
      "6618/6618 [==============================] - 6s 975us/step - loss: 0.1965 - precision: 0.8104 - val_loss: 0.1929 - val_precision: 0.8198\n",
      "Epoch 150/150\n",
      "6618/6618 [==============================] - 6s 980us/step - loss: 0.1967 - precision: 0.8104 - val_loss: 0.1937 - val_precision: 0.8111\n"
     ]
    }
   ],
   "source": [
    "batch = 512\n",
    "\n",
    "layers = 3\n",
    "\n",
    "# This runs the training. A for loop can be used to vary the parameters. \n",
    "model,kModel,startTime,modelName=runNN(layers,batch,0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Score =  0.800\n",
      " Signif =  1.40\n",
      " nsig = 124\n",
      " nbkg = 7876\n",
      "\n",
      "FileName                                                          ConfusionMatrix [TP FP] [FN TN]   Run Time               AUC     Avg.P   Score   Max Signif nsig        nbkg       \n",
      " 2021.02.18_11.29.10.numLayers3.numBranches75.batchSize512.CPU.h5  ([752562, 9142], [46045, 39251]) 0 days 00:17:33.168532  0.9025  0.6645   0.800   1.40             124        7876\n",
      "Saving model.....\n",
      "old auc: \n",
      " 0.9024670862391632 \n",
      " new auc 0.9025\n",
      "Model Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "aucroc=checkTraining(model,kModel)\n",
    "storeModel(model,startTime,modelName,aucroc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
